{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-shiv/ColorWall/blob/master/Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cizJeO2s8KNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8bff3f6d-db56-46cc-e4e5-e751104e215a"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"drive/My Drive/nlp\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "Ic4dXydZ71yR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c7affa73-1e0b-4dce-9a66-87f9b93cfd32"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "print(os.listdir(\".\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['web', '.github', '.vscode', 'data', 'CODE_OF_CONDUCT.md', '.gitignore', 'requirements.txt', '.DS_Store', 'LICENSE.md', 'test.py', 'CONFIG.py', 'README.md', 'utils.py', 'train.py', '.travis.yml', 'keras_model.py', 'Untitled0.ipynb', 'stringtoind', 'indtostring']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "OjX_4AIr71yW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_model(total_words, hidden_size, num_steps, optimizer='adam'):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    # Embedding layer / Input layer\n",
        "    model.add(tf.keras.layers.Embedding(\n",
        "        total_words, hidden_size, input_length=num_steps))\n",
        "\n",
        "    # 4 LSTM layers\n",
        "    model.add(tf.keras.layers.LSTM(units=hidden_size, return_sequences=True))\n",
        "    model.add(tf.keras.layers.LSTM(units=hidden_size, return_sequences=True))\n",
        "    model.add(tf.keras.layers.LSTM(units=hidden_size, return_sequences=True))\n",
        "    model.add(tf.keras.layers.LSTM(units=hidden_size, return_sequences=True))\n",
        "\n",
        "    # Fully Connected layer\n",
        "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1024)))\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.3, seed=0.2))\n",
        "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(512)))\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(tf.keras.layers.TimeDistributed(\n",
        "        tf.keras.layers.Dense(total_words)))\n",
        "    model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "                  metrics=[tf.keras.metrics.categorical_accuracy])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ejJRbnEO71ya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %load ../input/language/utils\n",
        "import collections\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "def load_dictionary(path):\n",
        "    return json.loads(open(path).read())\n",
        "\n",
        "\n",
        "def read_words(filename):\n",
        "    with tf.gfile.GFile(filename, 'r') as f:\n",
        "        return f.read().replace('\\n', '<eos>').split()\n",
        "\n",
        "\n",
        "def build_vocab(filename):\n",
        "    data = read_words(filename)\n",
        "\n",
        "    counter = collections.Counter(data)\n",
        "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "    words, _ = list(zip(*count_pairs))\n",
        "    word_to_id = dict(zip(words, range(len(words))))\n",
        "\n",
        "    return word_to_id\n",
        "\n",
        "\n",
        "def file_to_word_ids(filename, word_to_id):\n",
        "    data = read_words(filename)\n",
        "    return [word_to_id[word] for word in data if word in word_to_id]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    train_path = os.path.join('./data', 'ptb.train.txt')\n",
        "    valid_path = os.path.join('./data', 'ptb.valid.txt')\n",
        "\n",
        "    word_to_id = build_vocab(train_path)\n",
        "    train_data = file_to_word_ids(train_path, word_to_id)\n",
        "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
        "    total_words = len(word_to_id)\n",
        "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
        "    dictionary = {value: key for key, value in reversed_dictionary.items()}\n",
        "\n",
        "    print('\\ntotalwords : ', total_words, '\\n')\n",
        "    return train_data, valid_data, total_words, reversed_dictionary, dictionary\n",
        "\n",
        "\n",
        "def save_json(dictionary, filename):\n",
        "    with open(filename, 'w') as fp:\n",
        "        json.dump(dictionary, fp)\n",
        "\n",
        "\n",
        "class BatchGenerator(object):\n",
        "\n",
        "    def __init__(self, data, num_steps, batch_size, total_words, skip_step=5):\n",
        "        self.data = data\n",
        "        self.num_steps = num_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.total_words = total_words\n",
        "        self.current_idx = 0\n",
        "        self.skip_step = skip_step\n",
        "\n",
        "    def generate(self):\n",
        "        x = np.zeros((self.batch_size, self.num_steps))\n",
        "        y = np.zeros((self.batch_size, self.num_steps, self.total_words))\n",
        "        while True:\n",
        "            for i in range(self.batch_size):\n",
        "                if self.current_idx + self.num_steps >= len(self.data):\n",
        "                    self.current_idx = 0\n",
        "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
        "                temp_y = self.data[self.current_idx +\n",
        "                                   1:self.current_idx + self.num_steps + 1]\n",
        "                y[i, :, :] = tf.keras.utils.to_categorical(\n",
        "                    temp_y, num_classes=self.total_words)\n",
        "                self.current_idx += self.skip_step\n",
        "            yield x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ImlAJYLI71yd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2791
        },
        "outputId": "85693d23-8230-4c94-edb0-cbe4d4ce5a08"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "number_of_words = 3\n",
        "batch_size = 200\n",
        "hidden_size = 1500\n",
        "num_epochs = 42\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0\n",
        "\n",
        "# from keras_model import create_model\n",
        "# from utils import BatchGenerator, load_data, save_json\n",
        "\n",
        "train_data, valid_data, total_words, indexToString, stringToIndex = load_data()\n",
        "train_data = train_data[0:len(train_data)]\n",
        "\n",
        "print(len(train_data))\n",
        "train_data_generator = BatchGenerator(\n",
        "    train_data, number_of_words, batch_size, total_words, skip_step=number_of_words)\n",
        "valid_data_generator = BatchGenerator(\n",
        "    valid_data, number_of_words, batch_size, total_words, skip_step=number_of_words)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    lr=learning_rate, decay=learning_rate_decay)\n",
        "\n",
        "model = create_model(total_words=total_words, hidden_size=hidden_size,\n",
        "                     num_steps=number_of_words, optimizer=optimizer)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "save_json(stringToIndex,\"stringtoind\")\n",
        "\n",
        "save_json(indexToString, \"indtostring\")\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_data_generator.generate(),\n",
        "    steps_per_epoch=len(train_data)//(batch_size *\n",
        "                                      number_of_words),\n",
        "    epochs=num_epochs,\n",
        "    validation_data=valid_data_generator.generate(),\n",
        "    validation_steps=len(valid_data) //\n",
        "    (batch_size*number_of_words),\n",
        ")\n",
        "\n",
        "model.save(\"model.h5\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "totalwords :  10000 \n",
            "\n",
            "929589\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 3, 1500)           15000000  \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 3, 1500)           18006000  \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 3, 1500)           18006000  \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 3, 1500)           18006000  \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 3, 1500)           18006000  \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 3, 1024)           1537024   \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 3, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 1024)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 3, 512)            524800    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 3, 512)            0         \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 3, 10000)          5130000   \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 3, 10000)          0         \n",
            "=================================================================\n",
            "Total params: 94,215,824\n",
            "Trainable params: 94,215,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/42\n",
            "122/122 [==============================] - 10s 79ms/step - loss: 6.4226 - categorical_accuracy: 0.0930\n",
            "1549/1549 [==============================] - 393s 254ms/step - loss: 6.5477 - categorical_accuracy: 0.0787 - val_loss: 6.4226 - val_categorical_accuracy: 0.0930\n",
            "Epoch 2/42\n",
            "122/122 [==============================] - 9s 74ms/step - loss: 6.1723 - categorical_accuracy: 0.1198\n",
            "1549/1549 [==============================] - 389s 251ms/step - loss: 6.2903 - categorical_accuracy: 0.1061 - val_loss: 6.1723 - val_categorical_accuracy: 0.1198\n",
            "Epoch 3/42\n",
            "122/122 [==============================] - 9s 74ms/step - loss: 6.3119 - categorical_accuracy: 0.1094\n",
            "1549/1549 [==============================] - 388s 250ms/step - loss: 6.1295 - categorical_accuracy: 0.1193 - val_loss: 6.3119 - val_categorical_accuracy: 0.1094\n",
            "Epoch 4/42\n",
            "122/122 [==============================] - 9s 76ms/step - loss: 6.1087 - categorical_accuracy: 0.1279\n",
            "1549/1549 [==============================] - 387s 250ms/step - loss: 6.0531 - categorical_accuracy: 0.1227 - val_loss: 6.1087 - val_categorical_accuracy: 0.1279\n",
            "Epoch 5/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.3172 - categorical_accuracy: 0.1086\n",
            "1549/1549 [==============================] - 386s 249ms/step - loss: 5.9649 - categorical_accuracy: 0.1297 - val_loss: 6.3172 - val_categorical_accuracy: 0.1086\n",
            "Epoch 6/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.1299 - categorical_accuracy: 0.1318\n",
            "1549/1549 [==============================] - 385s 249ms/step - loss: 5.8679 - categorical_accuracy: 0.1361 - val_loss: 6.1299 - val_categorical_accuracy: 0.1318\n",
            "Epoch 7/42\n",
            "122/122 [==============================] - 9s 74ms/step - loss: 6.1013 - categorical_accuracy: 0.1382\n",
            "1549/1549 [==============================] - 384s 248ms/step - loss: 5.7550 - categorical_accuracy: 0.1448 - val_loss: 6.1013 - val_categorical_accuracy: 0.1382\n",
            "Epoch 8/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.2322 - categorical_accuracy: 0.1274\n",
            "1549/1549 [==============================] - 384s 248ms/step - loss: 5.7111 - categorical_accuracy: 0.1476 - val_loss: 6.2322 - val_categorical_accuracy: 0.1274\n",
            "Epoch 9/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.2596 - categorical_accuracy: 0.1254\n",
            "1549/1549 [==============================] - 384s 248ms/step - loss: 5.6860 - categorical_accuracy: 0.1479 - val_loss: 6.2596 - val_categorical_accuracy: 0.1254\n",
            "Epoch 10/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.2591 - categorical_accuracy: 0.1290\n",
            "1549/1549 [==============================] - 383s 247ms/step - loss: 5.6307 - categorical_accuracy: 0.1517 - val_loss: 6.2591 - val_categorical_accuracy: 0.1290\n",
            "Epoch 11/42\n",
            "122/122 [==============================] - 9s 75ms/step - loss: 6.1286 - categorical_accuracy: 0.1432\n",
            "1549/1549 [==============================] - 383s 247ms/step - loss: 5.5756 - categorical_accuracy: 0.1557 - val_loss: 6.1286 - val_categorical_accuracy: 0.1432\n",
            "Epoch 12/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.1243 - categorical_accuracy: 0.1461\n",
            "1549/1549 [==============================] - 383s 247ms/step - loss: 5.5411 - categorical_accuracy: 0.1576 - val_loss: 6.1243 - val_categorical_accuracy: 0.1461\n",
            "Epoch 13/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.1800 - categorical_accuracy: 0.1402\n",
            "1549/1549 [==============================] - 382s 247ms/step - loss: 5.4958 - categorical_accuracy: 0.1614 - val_loss: 6.1800 - val_categorical_accuracy: 0.1402\n",
            "Epoch 14/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.2626 - categorical_accuracy: 0.1373\n",
            "1549/1549 [==============================] - 382s 247ms/step - loss: 5.4884 - categorical_accuracy: 0.1603 - val_loss: 6.2626 - val_categorical_accuracy: 0.1373\n",
            "Epoch 15/42\n",
            "122/122 [==============================] - 9s 75ms/step - loss: 6.1871 - categorical_accuracy: 0.1452\n",
            "1549/1549 [==============================] - 382s 246ms/step - loss: 5.4370 - categorical_accuracy: 0.1644 - val_loss: 6.1871 - val_categorical_accuracy: 0.1452\n",
            "Epoch 16/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.2709 - categorical_accuracy: 0.1390\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.3946 - categorical_accuracy: 0.1672 - val_loss: 6.2709 - val_categorical_accuracy: 0.1390\n",
            "Epoch 17/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.2768 - categorical_accuracy: 0.1396\n",
            "1549/1549 [==============================] - 382s 246ms/step - loss: 5.3416 - categorical_accuracy: 0.1718 - val_loss: 6.2768 - val_categorical_accuracy: 0.1396\n",
            "Epoch 18/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.3613 - categorical_accuracy: 0.1347\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.3236 - categorical_accuracy: 0.1730 - val_loss: 6.3613 - val_categorical_accuracy: 0.1347\n",
            "Epoch 19/42\n",
            "122/122 [==============================] - 9s 74ms/step - loss: 6.1904 - categorical_accuracy: 0.1477\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.3045 - categorical_accuracy: 0.1748 - val_loss: 6.1904 - val_categorical_accuracy: 0.1477\n",
            "Epoch 20/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.4942 - categorical_accuracy: 0.1297\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.2988 - categorical_accuracy: 0.1740 - val_loss: 6.4942 - val_categorical_accuracy: 0.1297\n",
            "Epoch 21/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.2341 - categorical_accuracy: 0.1448\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.2028 - categorical_accuracy: 0.1826 - val_loss: 6.2341 - val_categorical_accuracy: 0.1448\n",
            "Epoch 22/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.6518 - categorical_accuracy: 0.1141\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.2322 - categorical_accuracy: 0.1793 - val_loss: 6.6518 - val_categorical_accuracy: 0.1141\n",
            "Epoch 23/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.3712 - categorical_accuracy: 0.1406\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.1316 - categorical_accuracy: 0.1892 - val_loss: 6.3712 - val_categorical_accuracy: 0.1406\n",
            "Epoch 24/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.4031 - categorical_accuracy: 0.1376\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.1646 - categorical_accuracy: 0.1856 - val_loss: 6.4031 - val_categorical_accuracy: 0.1376\n",
            "Epoch 25/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.4318 - categorical_accuracy: 0.1353\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.1523 - categorical_accuracy: 0.1851 - val_loss: 6.4318 - val_categorical_accuracy: 0.1353\n",
            "Epoch 26/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.2676 - categorical_accuracy: 0.1466\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.1647 - categorical_accuracy: 0.1840 - val_loss: 6.2676 - val_categorical_accuracy: 0.1466\n",
            "Epoch 27/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.3105 - categorical_accuracy: 0.1414\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.1786 - categorical_accuracy: 0.1817 - val_loss: 6.3105 - val_categorical_accuracy: 0.1414\n",
            "Epoch 28/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.3614 - categorical_accuracy: 0.1404\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.0917 - categorical_accuracy: 0.1897 - val_loss: 6.3614 - val_categorical_accuracy: 0.1404\n",
            "Epoch 29/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.6169 - categorical_accuracy: 0.1182\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.0883 - categorical_accuracy: 0.1895 - val_loss: 6.6169 - val_categorical_accuracy: 0.1182\n",
            "Epoch 30/42\n",
            "122/122 [==============================] - 9s 76ms/step - loss: 6.3855 - categorical_accuracy: 0.1384\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.0632 - categorical_accuracy: 0.1923 - val_loss: 6.3855 - val_categorical_accuracy: 0.1384\n",
            "Epoch 31/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.4135 - categorical_accuracy: 0.1382\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.0977 - categorical_accuracy: 0.1876 - val_loss: 6.4135 - val_categorical_accuracy: 0.1382\n",
            "Epoch 32/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.4211 - categorical_accuracy: 0.1377\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.0418 - categorical_accuracy: 0.1930 - val_loss: 6.4211 - val_categorical_accuracy: 0.1377\n",
            "Epoch 33/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.5436 - categorical_accuracy: 0.1280\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9682 - categorical_accuracy: 0.1994 - val_loss: 6.5436 - val_categorical_accuracy: 0.1280\n",
            "Epoch 34/42\n",
            "122/122 [==============================] - 9s 74ms/step - loss: 6.4887 - categorical_accuracy: 0.1301\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9926 - categorical_accuracy: 0.1965 - val_loss: 6.4887 - val_categorical_accuracy: 0.1301\n",
            "Epoch 35/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.5035 - categorical_accuracy: 0.1291\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9989 - categorical_accuracy: 0.1964 - val_loss: 6.5035 - val_categorical_accuracy: 0.1291\n",
            "Epoch 36/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.5618 - categorical_accuracy: 0.1293\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 5.0081 - categorical_accuracy: 0.1954 - val_loss: 6.5618 - val_categorical_accuracy: 0.1293\n",
            "Epoch 37/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.4767 - categorical_accuracy: 0.1364\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9646 - categorical_accuracy: 0.1992 - val_loss: 6.4767 - val_categorical_accuracy: 0.1364\n",
            "Epoch 38/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.3839 - categorical_accuracy: 0.1437\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9175 - categorical_accuracy: 0.2034 - val_loss: 6.3839 - val_categorical_accuracy: 0.1437\n",
            "Epoch 39/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.5478 - categorical_accuracy: 0.1311\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9248 - categorical_accuracy: 0.2023 - val_loss: 6.5478 - val_categorical_accuracy: 0.1311\n",
            "Epoch 40/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.6451 - categorical_accuracy: 0.1243\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9621 - categorical_accuracy: 0.1991 - val_loss: 6.6451 - val_categorical_accuracy: 0.1243\n",
            "Epoch 41/42\n",
            "122/122 [==============================] - 9s 73ms/step - loss: 6.5155 - categorical_accuracy: 0.1394\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9295 - categorical_accuracy: 0.2023 - val_loss: 6.5155 - val_categorical_accuracy: 0.1394\n",
            "Epoch 42/42\n",
            "122/122 [==============================] - 9s 72ms/step - loss: 6.5657 - categorical_accuracy: 0.1374\n",
            "1549/1549 [==============================] - 381s 246ms/step - loss: 4.9105 - categorical_accuracy: 0.2038 - val_loss: 6.5657 - val_categorical_accuracy: 0.1374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0mdNWkFe9auU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}